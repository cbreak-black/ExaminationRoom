\section{Design\label{Design}}
\paragraph{}
Implementing a flexible and powerful framework for user testing of stereoscopic problems requires a design that accounts for some special requirements. To find out what those requirements are, several tests where designed and executed. The design of the application was adapted to be fit for use in those and many other imaginable tests.

\paragraph{Creation}
The framework is split into three parts: The test design part aids with the construction of the test scene. A scene is a description of stimuli, possible inputs, and reactions to them. Traditional tests in this area show a series of pictures, requiring user feedback for each, while measuring the response time and the correctness. Stimuli are often precomputed and only consist of a single pixel image.

The requirements for stereoscopic test are different: It is not feasible to manually prepare stimuli, especially if complexe interaction is required. A method to generate them on the fly is needed.
Section \ref{sceneRep} describes how the look of a scene could be represented.

\paragraph{Testing}
The testing part is the most important. It displays stimuli and logs the user replies, reacting in a way that is defined by the designer of the test. While traditional tests only follow the question-response pattern, stereoscopic user tests often require more: Trials often have to be randomized with several constraints, simply randomizing might lead to overlaps, or imbalances in dependent properties. All parameters of the stimulus have to be directly or indirectly controllable based on user input such as movement of stimuli, movement of the camera, or visibility of objects.

\paragraph{Analysis}
Analysing the gathered data is the third part. This is usually done in a combinataion of \textit{Microsoft Excel} and highly specialized data analysation methods such as Anova. Both are sophisticated and well known by the users.

The easiest way to achieve compatibility with the known procedures is to produce data in a format, that is evaluatable in those tools. A log transformation program can take care of this in most tests: The time stamp based logs get parsed, and challenge-response times get measured and written in a format importable by spread sheet applications.
For more flexible data access, it can be imported into a database, where standard \textit{SQL} queries can be used to select subsets of the data, sorted and filtered in many ways.

Some tests might not follow this pattern, and the data has to be analysed in a different way.


\subsection{Representation of a Scene\label{sceneRep}}
\paragraph{}
\textit{Scene graphs} are used to store scenes in a structured way. They have been continually developed since their invention\cite{scenegraph} to account for spatial, state, hierarchy and other properties.
Many scene graph libraries exist, but they are usually focused on a specific field, and part of a bigger framework.

To be able to control inconsistencies such as individual depth cues, it is required that the scene graph can represent these. Nodes could add or remove cues for their children, giving a maximum of control to the users. Scene graph libraries do not support those requirements. Designing a simple scene graph is required.

\subsubsection{Node types}
\paragraph{}
Scene graphs are tree structures consisting of nodes with zero or more child nodes. The scene itself is typically the root node, while actual rendered objects often are leaves. The following nodes are planed to be implemented in the framework.

\begin{description}
\item[Rectangle] A primitive geometry node, which has the shape of a parallelogram. The node can reference a texture, which gets mapped on it's surface.
\item[Parallelepiped] A primitive geometry node, which has the shape of a parallelepiped (parallelogram prism). As it's parent it can be textured.
\item[Pixelplane] A primitive geometry node. It is used to draw pixel exact images at any position in space. It requires a texture to be visible. Pixelplanes are point shaped, and therefore are not influenced by scaling and rotation.
\item[Text] A primitive geometry node. It draws text at any position in space, similar to Pixelplanes.
\item[Mesh] A primitive geometry node who's geometry is defined by a mesh. The mesh can contain vertex normals and vertex texture coordinates. A texture can be mapped on the object. It might be desirable to also include vertex color.
\item[AffineTransformation] A group  node. It contains other nodes, which it can affine transform by directly manipulating the matrix stack.
\item[Camera] A group node. It contains other nodes, which are rendered by this camera instead of the global camera. This is used to control projection cues in a consistent way. Has to reset the depth buffer.
\item[DepthBuffer] A group node. Disables or enables the use of the depth buffer for all contained elements. This is used to disable occlusion clues. Can reset the depth buffer.
\item[Lighting] A group node. Enables lighting and a light source with controllable parameters for all sub nodes. This is used to enable lighting depth cues.
\item[Atmosphere] A group node. Enables the fog equation and controls its parameters. This is used to enable aerial depth cues.
\end{description}

\paragraph{}
The following nodes are problematic and might be hard to use, implement and understand due to their side effects.

\begin{description}
\item[XSize] A group node that scales objects to add or remove depth cues caused by perspective projection. It can be implemented on object (scale object individually, without consider intra object depth) or space level (scale  as function of depth, which is similar to a camera projection). Either way has it's own problems.
\item[XOffsest] A group node that moves objects it contains as function of their camera distance. Can be used to add or remove height-of-field, convergence or motion parallax cues.
\end{description}

\subsubsection{Node organisation}
\paragraph{}
Nodes are the building blocks of a scene graph, but how they are put together is similarly important. Due to the requirements of some nodes to clear the depth buffer, and the DepthBuffer node, the order of objects in a group node has to be in the desired order of drawing, not insertion.

Since most advanced functionality is designed to be in group nodes, objects have to be grouped based on their semantics. Those are drawn perspectively in the same camera node, all that have the be lit in a lighting node, and so on. For some nodes this is required for them to work, for others it is just a matter of performance.

To increase performance in complex scenes it might be desirable to use a spatially organized tree. Instead of grouping by state (such as camera, lighting), grouping would be done by proximity. This allows for faster culling. To get both functionalities, parallel trees have to be built. An object would reside in both trees. But so far, scene complexity is intended to be very low, so this optimisation is unlikely to be needed.


\subsection{Mechanics of a Scene\label{sceneMech}}
\paragraph{}
Having a scene with the desired visualisation is not enough. To conduct a test, the scene has to react to user input, to timers and has to be able to modify the scene graph. Logging has to be unconstrained. Randomisation and random permutations require a state.

\subsubsection{``Programming'' a scene}
\paragraph{}
The solution to all those problems is to use a programming language to define the scene mechanics. Anything that can be computed is assumed to be computable with a language that is \textit{turing complete}\cite{turing}. Using such a language would not limit the possibilities of the user, while making designing the scene much easier, faster and less error prone than writing a dedicated tool.

\subsubsection{Procedual API}
\paragraph{}
\textit{Procedural languages} are turing complete, so embedding a language would give all the computational flexibility needed. The same methods to build a scene graph and change object properties within \textit{C++} code can also be used in the scene mechanics. Proxy objects can mediate between the embedded language and the \textit{C++} API.

Creating scenes this way follows standard programming paradigmas, and someone who can program in a procedural language should have little problems adapting to what ever syntax the embedded language uses. The drawback of this approach is, that someone who does not have programming experience will find creating a scene rather unintuitive.

It is currently the only supported way of automatisation.
A possible alternative is discussed in the next section, but was not implemented.

\subsubsection{State API}
\paragraph{}
A more user friendly approach is \textit{Visual Programming}, a class of languages that are based on a visual instead of a textual representation of programs. Visual languages are used in data acquisition tools such as \textit{National Instruments}' \textit{LabVIEW}, music studio software such as \textit{Cycling '74}'s \textit{Max/MSP} or scripting tools like \textit{Apple}'s \textit{Automator}.
The pattern used in those languages is that of \textit{Dataflow-Programming}\cite{dataflow}. Unfortunately, it does not fit into the requirements of the testing tool.

Popular graphic languages in education are \textit{Finite State Machines}\cite{fsm}. They are easy to describe mathematically, and are rather intuitive, compared to traditional programming languages.
The original \textit{LEGO} \textit{Mindstorms} used a flow chart based language that is loosely related with Finite State Machines, and managed to appeal even to young people without programming experience.

A finite state machine is an algorithm that is described as set of states, and state transitions. The transitions can follow various rules, but the most common are based on input. A transition can also write output, or do various other things, depending on the type of state machine.
The original turing machine is a finite state machine that can also write on a so called ``tape'', and was of course turing complete.

Creating a new language, where every state represents a change in the scene, and every transition represents something that can be easily understood, such as a key press, or the passing of time, would allow for a relatively powerful but moderately easy to use way to define scene mechanics.
By moving interaction and memory writes and reads from the edges into the nodes, the use and implementation can be further simplified.

A good compromise between power and usability could look like \textit{flowcharts}, containing nodes that describe actions, nodes that represent decisions and end states.

\subsubsection{Flow types}
\paragraph{}
There can be several types of control flows to drive a scene:
It could start with the scene, and be executed stepwise until the scene and the flow ends.
The image would get drawn while the program waits for input, or for a timer.
This is closest to the traditional procedural programming paradigma.

An event based control flow could start each time a key is pressed, and execute the graph completely each time.
Other events like timers or mouse clicks can do the same.
This is closest to the event based model of the backend.

A concurrent flow could be an endless loop and yield control periodically, allowing custom animation.
Combining several kinds does not always make sense, but using separate lines of execution for animation and IO would simplify programming.

Some nodes can work differently in different flow contexts, those are actually different nodes that just share an idea and name.

\subsubsection{Flow Chart Nodes}
\paragraph{}
The flow chart state language would contain two basic primitives: Nodes and transitions.
Normal nodes have exactly one input and one output, only one transition can start at an output, but many can end at the same input. Start nodes have no input, end nodes no output. Decision nodes can have several outputs.

Each node can  execute code, access global variables, or change parameters of the scene.
As a tribute to data flow programming, nodes can pass a set of values to their successor.

\begin{description}
\item[Start]
A basic start node is unique, only one exists in each control flow.
As the name indicates, the program starts in this state.
Specialised start nodes are used in event driven flows.
It has no inputs and one output.

\item[End]
When this node is reached, the flow ends.
In a scene global model, the scene would terminate, and the test is completed.
In an event based model it would not be needed, but can serve as error check to ensure that all outputs are connected.

\item[Input]
Input nodes wait for keyboard or mouse input, and if it fulfills some basic properties, pass the value to their successor.
In an event based model, input nodes would be of the type start node, in a main flow a wait node.
The successor is usually a log node or a decision node.

\item[Timer]
A timer node continues the execution of a flow at a chosen time or time interval.
In an event based model timer nodes would start a new ``thread of execution'',
in a main flow it would pause the execution for the desired time.

\item[Log]
Log nodes write time stamped lines of arbitrary text to the log.

\item[Raw]
Raw nodes offer direct access to the underlying code, and should have the same power as interacting with the procedural API.
They are more difficult to use, and not intended for inexperienced users.

\item[Parameter]
A node type that sets the parameters of scene elements.
It offers the same control as the direct API.

\end{description}

Decision nodes help to guilde the flow of control.

\begin{description}
\item[If-then-else]
Evaluates a boolean expression, and continues the flow of execution depending on the result.

\item[Random]
The random node continues execution on one randomly chosen output flow.
Possible implementations include balanced random nodes, that only chose each path once, until each has been used once.

\end{description}


\subsection{User Interface}
\paragraph{}
While not strictly part of software design, the user interface design is never the less important\cite{hig}.
Even though the software backend is designed independently from the user interface, and can work independently of it, it is still influenced by requirements of the users, and the way of interaction.

The user interface of the test execution step is defined by the test itself, anything additional could conflict with the test and should be avoided.

The UI of the scene designer part however has the important function to help with the complicated task of scene design and automatisation. This includes the composition of the look of a scene, which elements it contains, and where they are placed, as well as the definition of the interaction, how the scene acts and reacts to user input.

\subsubsection{Scene}
Since the scene is internally a tree structure, using a standard tree view offers a powerful visualisation of it.
Tree like structures are widely used in many areas.
Most graphical file browser offer a tree view and all common 3D modeling programs visualize their object tree in a similar way.

Together with the layout of the scene, the type and parameters of objects should also be easily visible and changeable.
Again, widely used applications offer a solution: The parameters of the active entity is shown in detail in it's own widget area, and parameters can be changed there too.

The layout of the objects is controlled with drag\&drop. Parent/child relationships can easily be changed this way, and the paradigma of moving around items with the mouse is widely used in graphical user interfaces.

\subsubsection{Mechanics}
\todo{Tell about scene mechanics}

\paragraph{Other}
\todo{Key Bindings}
\todo{Menu}
\todo{Log File}
